A **transformer** is a [deep learning](https://en.wikipedia.org/wiki/Deep_learning "Deep learning") architecture developed by researchers at [Google](https://en.wikipedia.org/wiki/Google "Google") and based on the multi-head [attention](https://en.wikipedia.org/wiki/Attention_(machine_learning) "Attention (machine learning)") mechanism, proposed in the 2017 paper "[Attention Is All You Need](https://en.wikipedia.org/wiki/Attention_Is_All_You_Need "Attention Is All You Need")"

Transformers were developed specifically for seq2seq tasks like 
	1. Text Summarization
	2. Question and answering
	3. machine translation


# Attention Mechanism
## Masked Self Attention

